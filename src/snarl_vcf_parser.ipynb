{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from cyvcf2 import VCF\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from collections import defaultdict\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import fisher_exact\n",
    "import os\n",
    "import time\n",
    "    \n",
    "class Matrix :\n",
    "    def __init__(self, default_row_number=1000000, column_number=2):\n",
    "        self.default_row_number = default_row_number \n",
    "        self.matrix = np.zeros((default_row_number, column_number),dtype=bool)\n",
    "        self.row_header = None\n",
    "    \n",
    "    def get_matrix(self):\n",
    "        return self.matrix\n",
    "\n",
    "    def set_matrix(self, expended_matrix) :\n",
    "        self.matrix = expended_matrix\n",
    "\n",
    "    def get_row_header(self):\n",
    "        return self.row_header\n",
    "\n",
    "    def get_default_row_number(self):\n",
    "        return self.default_row_number\n",
    "\n",
    "    def set_row_header(self, row_header):\n",
    "        self.row_header = row_header  \n",
    "\n",
    "    def add_data(self, idx_snarl, idx_geno):\n",
    "        self.matrix[idx_snarl, idx_geno] = 1\n",
    "\n",
    "    def __str__(self) :\n",
    "        return f\"{self.row_header[0]} {self.matrix[0]} \\n\" \\\n",
    "               f\"{self.row_header[1]} {self.matrix[1]} \\n\" \\\n",
    "               f\"{self.row_header[2]} {self.matrix[2]} \\n\" \\\n",
    "               f\"{self.row_header[3]} {self.matrix[3]} \\n\" \\\n",
    "               f\"{self.row_header[4]} {self.matrix[4]} \\n\"\n",
    "\n",
    "class SnarlProcessor:\n",
    "    def __init__(self, vcf_path: str):\n",
    "        self.list_samples = VCF(vcf_path).samples\n",
    "        self.matrix = Matrix(1000000, len(self.list_samples)*2)\n",
    "        self.vcf_path = vcf_path\n",
    "\n",
    "    def expand_matrix(self):\n",
    "        \"\"\"\n",
    "        Expands a given numpy matrix by doubling the number of rows.\n",
    "        \"\"\"\n",
    "\n",
    "        data_matrix = self.matrix.get_matrix()\n",
    "        current_rows, current_cols = data_matrix.shape\n",
    "        new_rows = current_rows + self.matrix.get_default_row_number() # add + default_row_number row  \n",
    "\n",
    "        # Create a new matrix of zeros with the expanded size\n",
    "        expanded_matrix = np.zeros((new_rows, current_cols), dtype=data_matrix.dtype)\n",
    "        expanded_matrix[:current_rows, :] = data_matrix\n",
    "        self.matrix.set_matrix(expanded_matrix)\n",
    "            \n",
    "    def determine_str(self, s: str, length_s : int, i: int) -> tuple[int, int]:\n",
    "        \"\"\"Extract an integer from a string starting at index i.\"\"\"\n",
    "        start_idx = i\n",
    "        while i < length_s and s[i] not in ['>', '<']:\n",
    "            i += 1\n",
    "        return i, s[start_idx:i]\n",
    "\n",
    "    def decompose_string(self, s: str) -> List[str]:\n",
    "        \"\"\"Decompose a string with snarl information.\"\"\"\n",
    "        result = []\n",
    "        i = 0\n",
    "        length_s = len(s)\n",
    "        prev_int = None\n",
    "        prev_sym = None\n",
    "        \n",
    "        while i < length_s:\n",
    "            start_sym = s[i]\n",
    "            i += 1\n",
    "            i, current_int = self.determine_str(s, length_s, i)\n",
    "\n",
    "            if prev_int is not None and prev_sym is not None:\n",
    "                result.append(f\"{prev_sym}{prev_int}{start_sym}{current_int}\")\n",
    "            \n",
    "            prev_int = current_int\n",
    "            prev_sym = start_sym\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def decompose_snarl(self, lst: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Decompose a list of snarl strings.\"\"\"\n",
    "        return [self.decompose_string(s) for s in lst]\n",
    "\n",
    "    def get_or_add_index(self, ordered_dict, key, length_ordered_dict):\n",
    "        \"\"\" \n",
    "        Retrieve the index of the key if it exists in the OrderedDict.\n",
    "        If the key does not exist, add it and return the new index.\n",
    "        \"\"\"\n",
    "        if key in ordered_dict:\n",
    "            return ordered_dict[key]\n",
    "        else:\n",
    "            new_index = length_ordered_dict\n",
    "            ordered_dict[key] = new_index\n",
    "            return new_index\n",
    "    \n",
    "    def push_matrix(self, idx_snarl, decomposed_snarl, allele, row_header_dict, index_column):\n",
    "        \"\"\"Add True to the matrix if snarl is found\"\"\"\n",
    "\n",
    "        current_rows_number, _ = self.matrix.get_matrix().shape\n",
    "\n",
    "        if allele == idx_snarl:\n",
    "            # Retrieve or add the index in one step and calculate the length once\n",
    "            length_ordered_dict = len(row_header_dict)\n",
    "            idx_snarl = self.get_or_add_index(row_header_dict, decomposed_snarl, length_ordered_dict)\n",
    "\n",
    "            # Check if a new matrix chunk is needed (only if length > 1)\n",
    "            if length_ordered_dict > 1 and length_ordered_dict > current_rows_number -1 :\n",
    "                self.expand_matrix()\n",
    "\n",
    "            # Compute index row\n",
    "            row_index = idx_snarl\n",
    "            \n",
    "            # Add data to the matrix\n",
    "            self.matrix.add_data(row_index, index_column)\n",
    "\n",
    "    def truncate_matrix(self, max_rows):\n",
    "        \"\"\"\n",
    "        Truncates the matrix to a specified maximum number of rows.\n",
    "        \"\"\"\n",
    "\n",
    "        data_matrix = self.matrix.get_matrix()\n",
    "        current_rows, _ = data_matrix.shape\n",
    "        if max_rows == current_rows:\n",
    "            return data_matrix\n",
    "\n",
    "        truncated_matrix = data_matrix[:max_rows, :]\n",
    "\n",
    "        return truncated_matrix\n",
    "\n",
    "    def fill_matrix(self):\n",
    "        \"\"\"Parse VCF file (main function)\"\"\"\n",
    "        row_header_dict = dict()\n",
    "\n",
    "        # Parse variant line by line\n",
    "        for variant in VCF(self.vcf_path):\n",
    "            genotypes = variant.genotypes  # Extract genotypes once per variant\n",
    "            snarl_list = variant.INFO.get('AT', '').split(',')  # Extract and split snarl list once per variant\n",
    "            list_list_decomposed_snarl = self.decompose_snarl(snarl_list)  # Decompose snarls once per variant\n",
    "\n",
    "            # Loop over each decomposed snarl list and genotype\n",
    "            for idx_snarl, list_decomposed_snarl in enumerate(list_list_decomposed_snarl):\n",
    "                for decomposed_snarl in list_decomposed_snarl:\n",
    "                    # Loop over genotypes with index_column tracking\n",
    "                    for index_column, genotype in enumerate(genotypes):\n",
    "                        allele_1, allele_2 = genotype[:2]  # Extract alleles\n",
    "                        \n",
    "                        # Push matrix only if both alleles are valid\n",
    "                        if allele_1 != -1 and allele_2 != -1:\n",
    "                            col_idx = index_column * 2\n",
    "                            self.push_matrix(idx_snarl, decomposed_snarl, allele_1, row_header_dict, col_idx)\n",
    "                            self.push_matrix(idx_snarl, decomposed_snarl, allele_2, row_header_dict, col_idx + 1)\n",
    "\n",
    "        self.matrix.set_row_header(row_header_dict)\n",
    "\n",
    "    def check_pheno_group(self, group) :\n",
    "        \"\"\"Check if all sample name in the matrix are matching with phenotype else return error\"\"\"\n",
    "        if type(group) == tuple :\n",
    "            list_group = group[0] + group[1]\n",
    "        elif type(group) == dict :\n",
    "            list_group = [i for i in group.keys()]\n",
    "        else :\n",
    "            raise ValueError(f\"group type : {type(group)} not an dict or a tuple.\")\n",
    "\n",
    "        set_sample = set(self.list_samples)\n",
    "        set_group = set(list_group)\n",
    "        missing_elements = set_sample - set_group\n",
    "\n",
    "        if missing_elements:\n",
    "            raise ValueError(f\"The following elements from set_sample are not present in set_group: {missing_elements}\")\n",
    "\n",
    "    def binary_table(self, snarls, binary_groups, output=\"output/binary_output.tsv\") : \n",
    "\n",
    "        #self.check_pheno_group(binary_groups)\n",
    "        idx = 0\n",
    "        with open(output, 'wb') as outf:\n",
    "\n",
    "            headers = 'Snarl\\tP_value (Fisher)\\tP_value (Chi2)\\tTable_sum\\tInter_group\\tAverage\\n'\n",
    "            outf.write(headers.encode('utf-8'))\n",
    "            for snarl, list_snarl in snarls.items() :\n",
    "                print(\"idx : \", idx)\n",
    "                df = self.create_binary_table(binary_groups, list_snarl)\n",
    "                fisher_p_value, chi2_p_value, total_sum, numb_colum, inter_group, average = self.binary_stat_test(df)\n",
    "                data = '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(snarl, fisher_p_value, chi2_p_value, total_sum, numb_colum, inter_group, average)\n",
    "                outf.write(data.encode('utf-8'))\n",
    "                idx += 1\n",
    "                if idx > 5 :\n",
    "                    break\n",
    "\n",
    "    def quantitative_table(self, snarls, quantitative, output=\"output/quantitative_output.tsv\") :\n",
    "\n",
    "        #self.check_pheno_group(quantitative)\n",
    "        idx = 0\n",
    "\n",
    "        with open(output, 'wb') as outf:\n",
    "            headers = 'Snarl\\tP_value\\n'\n",
    "            outf.write(headers.encode('utf-8'))\n",
    "            \n",
    "            for _, snarl in snarls.items() :\n",
    "                print(\"idx : \", idx)\n",
    "\n",
    "                df = self.create_quantitative_table(snarl)\n",
    "                snarl, pvalue = self.linear_regression(df, quantitative)\n",
    "                # nb column\n",
    "                # min sum column\n",
    "                data = '{}\\t{}\\n'.format(snarl, pvalue)\n",
    "                outf.write(data.encode('utf-8'))\n",
    "                idx += 1\n",
    "                if idx > 5 :\n",
    "                    break\n",
    "\n",
    "    def identify_correct_path(self, decomposed_snarl: list, row_headers_dict: dict, idx_srr_save: list) -> list:\n",
    "        \"\"\"\n",
    "        Return a list of column index where all specifique element of this column of matrix are 1\n",
    "        \"\"\"\n",
    "        matrix = self.matrix.get_matrix()\n",
    "        rows_to_check = []\n",
    "        \n",
    "        for snarl in decomposed_snarl :\n",
    "            if \"*\" not in snarl and snarl in row_headers_dict :\n",
    "                idx_row = row_headers_dict[snarl]\n",
    "                rows_to_check.append(idx_row)\n",
    "            else:\n",
    "                return [] \n",
    "            \n",
    "        if not rows_to_check :\n",
    "            return []\n",
    "\n",
    "        rows_to_check = np.array(rows_to_check)\n",
    "        extracted_rows = matrix[rows_to_check, :]\n",
    "        columns_all_ones = np.all(extracted_rows == 1, axis=0)\n",
    "        idx_srr_save = np.where(columns_all_ones)[0].tolist()\n",
    "        \n",
    "        return idx_srr_save\n",
    "\n",
    "    def create_binary_table(self, groups, list_path_snarl) -> pd.DataFrame :\n",
    "        \"\"\"Generates a binary table DataFrame indicating the presence of snarl paths in given groups based on matrix data\"\"\"\n",
    "        row_headers_dict = self.matrix.get_row_header()\n",
    "        list_samples = self.list_samples\n",
    "        length_column_headers = len(list_path_snarl)\n",
    "\n",
    "        # Initialize g0 and g1 with zeros, corresponding to the length of column_headers\n",
    "        g0 = [0] * length_column_headers\n",
    "        g1 = [0] * length_column_headers\n",
    "\n",
    "        # Iterate over each path_snarl in column_headers\n",
    "        for idx_g, path_snarl in enumerate(list_path_snarl):\n",
    "            idx_srr_save = list(range(len(list_samples)))\n",
    "            decomposed_snarl = self.decompose_string(path_snarl)\n",
    "            idx_srr_save = self.identify_correct_path(decomposed_snarl, row_headers_dict, idx_srr_save)\n",
    "            \n",
    "            # Count occurrences in g0 and g1 based on the updated idx_srr_save\n",
    "            for idx in idx_srr_save :\n",
    "                srr = list_samples[idx//2]\n",
    "\n",
    "                if srr in groups[0]:\n",
    "                    g0[idx_g] += 1\n",
    "                if srr in groups[1]:  \n",
    "                    g1[idx_g] += 1\n",
    "\n",
    "        # Create and return the DataFrame\n",
    "        df = pd.DataFrame([g0, g1], index=['G0', 'G1'], columns=list_path_snarl)\n",
    "        return df\n",
    "\n",
    "    def create_quantitative_table(self, column_headers : list) -> pd.DataFrame:\n",
    "        row_headers_dict = self.matrix.get_row_header()\n",
    "        column_headers_header = self.list_samples\n",
    "        genotypes = []\n",
    "\n",
    "        # Iterate over each path_snarl in column_headers\n",
    "        for path_snarl in column_headers:\n",
    "            idx_srr_save = list(range(len(column_headers_header)))\n",
    "            decomposed_snarl = self.decompose_string(path_snarl)\n",
    "            idx_srr_save = self.identify_correct_path(decomposed_snarl, row_headers_dict, idx_srr_save)\n",
    "\n",
    "            genotype = [1 if idx in idx_srr_save else 0 for idx in range(len(column_headers_header))]\n",
    "            genotypes.append(genotype)\n",
    "\n",
    "        # Transposing the matrix\n",
    "        transposed_genotypes = list(map(list, zip(*genotypes)))\n",
    "        df = pd.DataFrame(transposed_genotypes, index=column_headers_header, columns=column_headers)\n",
    "        return df\n",
    "\n",
    "    def linear_regression(self, df, pheno : dict) -> float :\n",
    "        \n",
    "        df = df.astype(int)\n",
    "        df['Target'] = df.index.map(pheno)\n",
    "\n",
    "        x = df.drop('Target', axis=1)\n",
    "        y = df['Target']\n",
    "\n",
    "        # Fit the regression model\n",
    "        x = sm.add_constant(x)\n",
    "        result = sm.OLS(y, x).fit()\n",
    "\n",
    "        # Extract p-values from the fitted model and format as a list of tuples\n",
    "        index, pval = next(iter(result.pvalues.items()))\n",
    "        print(\"result.summary() : \", result.summary())\n",
    "        if str(pval) == 'nan':\n",
    "            pval = \"N/A\"\n",
    "        return index, pval\n",
    "\n",
    "    def chi2_test(self, df) -> float:\n",
    "        \"\"\"Calculate p_value from list of dataframe using chi-2 test\"\"\"\n",
    "\n",
    "        # Check if dataframe has at least 2 columns and more than 0 counts in every cell\n",
    "        if df.shape[1] >= 2 and np.all(df.sum(axis=0)) and np.all(df.sum(axis=1)):\n",
    "            try:\n",
    "                # Perform Chi-Square test\n",
    "                chi2, p_value, dof, expected = chi2_contingency(df)\n",
    "            except ValueError as e:\n",
    "                p_value = \"Error\"\n",
    "        else:\n",
    "            p_value = \"N/A\"\n",
    "\n",
    "        return p_value\n",
    "\n",
    "    def fisher_test(self, df) -> float : \n",
    "        \"\"\"Calcul p_value using fisher exact test\"\"\"\n",
    "\n",
    "        try:\n",
    "            odds_ratio, p_value = fisher_exact(df)\n",
    "\n",
    "        except ValueError as e: \n",
    "            p_value = 'N/A'\n",
    "        \n",
    "        return p_value\n",
    "     \n",
    "    def binary_stat_test(self, df) :\n",
    "\n",
    "        fisher_p_value = self.fisher_test(df)\n",
    "        chi2_p_value = self.chi2_test(df)\n",
    "        total_sum = int(df.values.sum())\n",
    "        inter_group = int(df.min().sum())\n",
    "        numb_colum = df.shape[1]\n",
    "        average = float(total_sum / numb_colum)\n",
    "        return fisher_p_value, chi2_p_value, total_sum, numb_colum, inter_group, average\n",
    "\n",
    "def parse_group_file(group_file : str):\n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(group_file, sep='\\t')\n",
    "    \n",
    "    # Check if the required columns are present\n",
    "    required_headers = {'sample', 'group'}\n",
    "    if not required_headers.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing required columns. The file must contain the following columns: {required_headers}\")\n",
    "    \n",
    "    # Extract samples belonging to group 0 and group 1\n",
    "    group_0 = df[df['group'] == 0]['sample'].tolist()\n",
    "    group_1 = df[df['group'] == 1]['sample'].tolist()\n",
    "    \n",
    "    return group_0, group_1\n",
    " \n",
    "def parse_pheno_file(file_path : str) -> dict:\n",
    "    # Read the file into a DataFrame\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    # Extract the IID (second column) and PHENO (third column) and convert PHENO to float\n",
    "    parsed_pheno = dict(zip(df['IID'], df['PHENO']))\n",
    "\n",
    "    return parsed_pheno\n",
    "\n",
    "def parse_snarl_path_file(path_file: str) -> dict:\n",
    "    # Initialize a defaultdict with lists as default values\n",
    "    snarl_paths = defaultdict(list)\n",
    "    \n",
    "    # Read the file using pandas\n",
    "    df = pd.read_csv(path_file, sep='\\t', dtype=str)\n",
    "    \n",
    "    # Iterate through the DataFrame rows\n",
    "    for _, row in df.iterrows():\n",
    "        snarl = row['snarl']\n",
    "        paths = row['paths']\n",
    "        \n",
    "        # Split paths by comma and add them to the defaultdict\n",
    "        if pd.notna(paths):\n",
    "            path_list = paths.split(',')\n",
    "            snarl_paths[snarl].extend(path_list)\n",
    "    \n",
    "    return snarl_paths\n",
    "\n",
    "def check_format_vcf_file(file_path : str) -> str:\n",
    "    \"\"\"\n",
    "    Function to check if the provided file path is a valid VCF file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise argparse.ArgumentTypeError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "    if not file_path.lower().endswith('.vcf') and not file_path.lower().endswith('.vcf.gz'):\n",
    "        raise argparse.ArgumentTypeError(f\"The file {file_path} is not a valid VCF file. It must have a .vcf extension or .vcf.gz.\")\n",
    "    return file_path\n",
    "\n",
    "def check_format_group_snarl(file_path : str) -> str :\n",
    "    \"\"\"\n",
    "    Function to check if the provided file path is a valid group file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise argparse.ArgumentTypeError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    if not file_path.lower().endswith('.txt') and not file_path.lower().endswith('.tsv'):\n",
    "        raise argparse.ArgumentTypeError(f\"The file {file_path} is not a valid group/snarl file. It must have a .txt extension or .tsv.\")\n",
    "    return file_path\n",
    "    \n",
    "def check_format_pheno(file_path : str) -> str :\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        first_line = file.readline().strip()\n",
    "    \n",
    "    header = first_line.split('\\t')\n",
    "    expected_header = ['FID', 'IID', 'PHENO']\n",
    "    if header != expected_header:\n",
    "        raise ValueError(f\"The file must contain the following headers: {expected_header} and be split by tabulation\")\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "vcf_path = \"/home/mbagarre/Bureau/snarl_data/fly.merged.vcf\"\n",
    "snarl_path = \"/home/mbagarre/Bureau/snarl_data/list_snarl_output.txt\"\n",
    "vcf_object = SnarlProcessor(vcf_path)\n",
    "vcf_object.fill_matrix()\n",
    "print(f\"Time Matrix : {time.time() - start} s\")\n",
    "snarl = parse_snarl_path_file(snarl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "binary_path = \"/home/mbagarre/Bureau/snarl_data/group.txt\"\n",
    "output = \"/home/mbagarre/Bureau/Projet/snarl_project/output/binary_output.tsv\"\n",
    "binary_group = parse_group_file(binary_path)\n",
    "vcf_object.binary_table(snarl, binary_group, output)\n",
    "\n",
    "print(f\"Time P-value: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mbagarre/Bureau/Projet/snarl_project/output/quantitative_output.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m quantitative \u001b[38;5;241m=\u001b[39m parse_pheno_file(quantitative_path)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mvcf_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantitative_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43msnarl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantitative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime P-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 214\u001b[0m, in \u001b[0;36mSnarlProcessor.quantitative_table\u001b[0;34m(self, snarls, quantitative, output)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, snarl \u001b[38;5;129;01min\u001b[39;00m snarls\u001b[38;5;241m.\u001b[39mitems() :\n\u001b[1;32m    213\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_quantitative_table(snarl)\n\u001b[0;32m--> 214\u001b[0m     snarl, pvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantitative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# nb column\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# min sum column\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(snarl, pvalue)\n",
      "Cell \u001b[0;32mIn[1], line 305\u001b[0m, in \u001b[0;36mSnarlProcessor.linear_regression\u001b[0;34m(self, df, pheno)\u001b[0m\n\u001b[1;32m    302\u001b[0m result \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(y, x)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Extract p-values from the fitted model and format as a list of tuples\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m index, pval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpvalues\u001b[49m\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m#print(\"result.summary() : \", result.summary())\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pval) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/statsmodels/base/wrapper.py:40\u001b[0m, in \u001b[0;36mResultsWrapper.__getattribute__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     38\u001b[0m     obj \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mwrap_output(obj, how[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39mhow[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how:\n\u001b[0;32m---> 40\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/statsmodels/base/data.py:440\u001b[0m, in \u001b[0;36mModelData.wrap_output\u001b[0;34m(self, obj, how, names)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattach_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_rows(obj)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/statsmodels/base/data.py:560\u001b[0m, in \u001b[0;36mPandasData.attach_columns\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattach_columns\u001b[39m(\u001b[38;5;28mself\u001b[39m, result):\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# this can either be a 1d array or a scalar\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# do not squeeze because it might be a 2d row array\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# if it needs a squeeze, the bug is elsewhere\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 560\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# for e.g., confidence intervals\u001b[39;00m\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(result, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_names)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:490\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    487\u001b[0m name \u001b[38;5;241m=\u001b[39m ibase\u001b[38;5;241m.\u001b[39mmaybe_extract_name(name, data, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 490\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dtype(dtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:7647\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7645\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiIndex\u001b[38;5;241m.\u001b[39mfrom_arrays(index_like)\n\u001b[1;32m   7646\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtupleize_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   7648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    562\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:664\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n\u001b[0;32m--> 664\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_str_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subarr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/construction.py:744\u001b[0m, in \u001b[0;36m_sanitize_str_dtypes\u001b[0;34m(result, data, dtype, copy)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03mEnsure we have a dtype that is supported by pandas.\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# This is to prevent mixed-type Series getting all casted to\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# NumPy string type, e.g. NaN --> '-1#IND'.\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;66;03m# GH#16605\u001b[39;00m\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;66;03m# If not empty convert the data to dtype\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# GH#19853: If data is a scalar, result has already the result\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(data):\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(isna(data)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "quantitative_path = \"/home/mbagarre/Bureau/snarl_data/updated_phenotypes.txt\"\n",
    "output = \"/home/mbagarre/Bureau/Projet/snarl_project/output/quantitative_output.tsv\"\n",
    "quantitative = parse_pheno_file(quantitative_path)\n",
    "vcf_object.quantitative_table(snarl, quantitative, output)\n",
    "\n",
    "print(f\"Time P-value: {time.time() - start} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
